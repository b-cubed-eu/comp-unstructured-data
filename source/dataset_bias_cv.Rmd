---
title: "Investigate dataset bias"
author: "Ward Langeraert"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    toc_collapsed: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
# Load packages
library(tidyverse) # Data wrangling and visualisation
library(dubicube)  # Cross-validation
library(sf)        # Spatial objects
library(targets)

# Globals
store <- here::here("source", "pipelines", "exploratory_analysis", "_targets")
out_path <- here::here("output", "dataset_bias_cv")
dir.create(out_path, recursive = TRUE, showWarnings = FALSE)

# Source functions
source(here::here("source", "R", "download_occ_cube.R"))
source(here::here("source", "R", "get_dataset_names.R"))
source(here::here("source", "R", "plot_cross_validation.R"))
```

# Goal

Investigate bias in indicators caused by the over/under representation of certain datasets in the occurrence cube.

# Load data

We download the occurrence cube.
This cube is four dimensional, where the fourth dimension is the dataset.

```{r, message=FALSE}
# nolint start: line_length_linter.
query <- "SELECT
  \"year\",
  GBIF_MGRSCode(1000, decimalLatitude, decimalLongitude,
  COALESCE(coordinateUncertaintyInMeters, 1000)) AS mgrsCode,
  speciesKey,
  species,
  family,
  datasetName,
  datasetKey,
  COUNT(*) AS n,
  MIN(COALESCE(coordinateUncertaintyInMeters, 1000)) AS minCoordinateUncertaintyInMeters,
  IF(ISNULL(family), NULL, SUM(COUNT(*)) OVER (PARTITION BY family)) AS familyCount
  FROM
  occurrence
  WHERE
  occurrenceStatus = 'PRESENT'
  AND NOT ARRAY_CONTAINS(issue, 'ZERO_COORDINATE')
  AND NOT ARRAY_CONTAINS(issue, 'COORDINATE_OUT_OF_RANGE')
  AND NOT ARRAY_CONTAINS(issue, 'COORDINATE_INVALID')
  AND NOT ARRAY_CONTAINS(issue, 'COUNTRY_COORDINATE_MISMATCH')
  AND level1gid = 'BEL.2_1'
  AND \"year\" >= 2007
  AND \"year\" <= 2022
  AND speciesKey IS NOT NULL
  AND decimalLatitude IS NOT NULL
  AND decimalLongitude IS NOT NULL
  AND class = 'Aves'
  AND collectionCode != 'ABV'
  GROUP BY
  \"year\",
  mgrsCode,
  speciesKey,
  species,
  family,
  datasetName,
  datasetKey
  ORDER BY
  \"year\" ASC,
  mgrsCode ASC,
  speciesKey ASC,
  datasetName ASC"
# nolint end

download_occ_cube(
  sql_query = query,
  file = "birdcubeflanders_dataset.csv",
  path = here::here("data", "raw"),
  overwrite = FALSE
)
```

> GBIF.org (05 August 2025) GBIF Occurrence Download https://doi.org/10.15468/dl.48vfzy

We read in the data cube and add dataset names.

```{r}
birdcubeflanders_dataset_raw <- read_csv(
  here::here("data", "raw", "birdcubeflanders_dataset.csv"),
  show_col_types = FALSE
)

# Add dataset names
birdcubeflanders_dataset <- get_dataset_names(birdcubeflanders_dataset_raw) %>%
  dplyr::select("mgrscode", "year", "specieskey", "species", "family",
                "datasetkey", "datasetname", "n",
                "mincoordinateuncertaintyinmeters", "familycount") %>%
  arrange(year, mgrscode, species, datasetname)

glimpse(birdcubeflanders_dataset)
```

There are `r nrow(birdcubeflanders_dataset)` rows in the dataset.
They should be the same as the unique combinations of grid cell code, species, year and dataset.

```{r}
n_distinct <- birdcubeflanders_dataset %>%
  distinct(mgrscode, specieskey, year, datasetname) %>%
  nrow()

nrow(birdcubeflanders_dataset) == n_distinct
```

In total, there are `r n_distinct(birdcubeflanders_dataset$datasetname)` component datasets. We look at the number and proportion of observations per dataset in the cube.

```{r}
total_obs <- sum(birdcubeflanders_dataset$n)
birdcubeflanders_dataset %>%
  summarise(n_obs = sum(n),
            p_obs = n_obs / total_obs,
            .by = "datasetname") %>%
  arrange(desc(n_obs)) %>%
  knitr::kable()
```

```{r}
birdcubeflanders_dataset %>%
  summarise(n_obs = sum(n),
            .by = "datasetname") %>%
  mutate(datasetname = reorder(datasetname, n_obs)) %>%
  ggplot(aes(x = datasetname, y = n_obs)) +
  geom_bar(stat = "identity",
           fill = "cornflowerblue") +
  geom_text(aes(label = n_obs), vjust = 0.3, hjust = -0.3, size = 3) +
  scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.2))) +
  labs(x = "", y = "Number of observations") +
  theme_bw(base_size = 12) +
  theme(plot.margin = unit(c(0.2, 0.5, 0.2, 0.2), 'cm')) +
  coord_flip()
```

We look at the number and proportion of species per dataset in the cube.

```{r}
total_spec <- n_distinct(birdcubeflanders_dataset$species)
birdcubeflanders_dataset %>%
  summarise(n_spec = n_distinct(species),
            p_spec = n_spec / total_spec,
            .by = "datasetname") %>%
  arrange(desc(n_spec)) %>%
  knitr::kable()
```

```{r}
birdcubeflanders_dataset %>%
  summarise(n_spec = n_distinct(species),
            .by = "datasetname") %>%
  mutate(datasetname = reorder(datasetname, n_spec)) %>%
  ggplot(aes(x = datasetname, y = n_spec)) +
  geom_bar(stat = "identity",
           fill = "cornflowerblue") +
  geom_text(aes(label = n_spec), vjust = 0.3, hjust = -0.3, size = 3) +
  scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.2))) +
  labs(x = "", y = "Number of species") +
  theme_bw(base_size = 12) +
  theme(plot.margin = unit(c(0.2, 0.5, 0.2, 0.2), 'cm')) +
  coord_flip()
```

We see a large number of observations in a small number of component datasets. Some datasets are specialised in specific species, others are general.

```{r}
cat_labs <- c(n_obs = "Number of observations", n_spec = "Number of species")

p_dataset <- birdcubeflanders_dataset %>%
  summarise(n_obs = sum(n),
            n_spec = n_distinct(species),
            .by = "datasetname") %>%
  mutate(datasetname = reorder(datasetname, n_obs)) %>%
  pivot_longer(cols = c("n_obs", "n_spec"),
               names_to = "cat", values_to = "n") %>%
  ggplot(aes(x = datasetname, y = n)) +
  geom_bar(stat = "identity",
           fill = "cornflowerblue") +
  geom_text(aes(label = n), vjust = 0.3, hjust = -0.1, size = 2.5) +
  scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.2))) +
  labs(x = "", y = "") +
  facet_wrap(~cat, ncol = 2, scales = "free_x",
             labeller = labeller(cat = cat_labs)) +
  theme_bw(base_size = 12) +
  theme(axis.title.x = element_blank(),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank()) +
  coord_flip()
p_dataset
```

```{r}
ggsave(file.path(out_path, "component_datasets.png"),
       p_dataset,
       width = 10, height = 8, dpi = 300)
```

# Comparing species prevalence
## Load and process data

We load the ABV data (structured monitoring data).
We categorise the species according to rarity:

| number of rows  | rarity           |
| :-------------: | :--------------- |
| [1, 10)         | very rare        |
| [10, 100)       | rare             |
| [100, 1000)     | common           |
| [1000, 10.000)  | very common      |
| [10.000, +Inf)  | extremely common |

```{r, message=FALSE}
abv_data_total <- tar_read(data, store = store)

# Cut rarity
abv_data <- abv_data_total %>%
  filter(id_dataset == "abv_data",
         id_spat_res == "1km") %>%
  rename("rarity" = "category") %>%
  arrange(year, mgrscode, species)

glimpse(abv_data)
```

We filter the data cube based on species in ABV and add rarity categories.

```{r}
# Get ABV categories
abv_species <- unique(abv_data$species)

# Filter cube based on ABV species and add rarity categories
birdcube_dataset_filtered <- birdcubeflanders_dataset %>%
  dplyr::filter(species %in% abv_species) %>%
  left_join(distinct(abv_data[, c("species", "rarity")]),
            by = join_by("species"))

# Also aggregate over datasets
birdcube_filtered <- birdcube_dataset_filtered %>%
  group_by(mgrscode, year, specieskey, species, family, familycount, rarity) %>%
  summarise(
    n = sum(n),
    mincoordinateuncertaintyinmeters = min(mincoordinateuncertaintyinmeters),
    .groups = "drop"
  )
```

## Indicator calculation

We calculate species prevalence.
For each dataset we calculate the proportion of occupied grid cells by each species.

```{r}
species_prevalence <- function(
    df,
    total_grid_cells = length(unique(df$mgrscode))) {
  require("dplyr")
  require("rlang")

  df_out <- df %>%
    group_by(.data$species, .data$rarity) %>%
    summarise(diversity_val = n_distinct(.data$mgrscode) / total_grid_cells,
              .groups = "drop")

  return(df_out)
}
```

```{r}
birdcube_prevalence <- species_prevalence(birdcube_filtered) %>%
  mutate(data = "birdcube")
abv_prevalence <- species_prevalence(abv_data) %>%
  mutate(data = "abv")

prevalence_df <- bind_rows(birdcube_prevalence, abv_prevalence) %>%
  pivot_wider(names_from = data, values_from = diversity_val)
```

Rare species are more prevalent in the cube dataset while more common species are more prevalent in the ABV dataset.

```{r}
p_prevalence <- prevalence_df %>%
  ggplot(aes(x = abv, y = birdcube)) +
  geom_abline(slope = 1, intercept = 0, colour = "firebrick",
              linewidth = 1) +
  annotate("label", x = 0.8, y = 0.6, size = 3,
           label = "Higher prevalence in ABV",
           color = "black") +
  annotate("label", x = 0.4, y = 0.6, size = 3,
           label = "Higher prevalence in cube",
           color = "black") +
  geom_smooth(method = "loess", formula = "y ~ x",
              colour = "darkgrey", linetype = "dashed") +
  geom_point(aes(shape = rarity), size = 2) +
  labs(x = "Proportion of occupied grid cells\nin ABV dataset",
       y = "Proportion of occupied grid cells\nin cube dataset",
       shape = "Rarity") +
  theme_bw(base_size = 12)
p_prevalence
```

```{r}
ggsave(file.path(out_path, "prevalence.png"),
       p_prevalence,
       width = 8, height = 6, dpi = 300)
```

We calculate error measures for the indicator based on leave-one-dataset-out cross-validation.
We use a constant total of grid cells (`r length(unique(birdcube_dataset_filtered$mgrscode))`) such that this is independent from the datasets left out.

```{r, message=FALSE}
prevalence_cv <- cross_validate_cube(
  birdcube_dataset_filtered,
  fun = species_prevalence,
  total_grid_cells = length(unique(birdcube_dataset_filtered$mgrscode)),
  grouping_var = c("species", "rarity"),
  out_var = "datasetkey",
  progress = TRUE,
  processed_cube = FALSE
)
```

```{r}
quantile <- 0.95
```

We visualise several error measures per species. Labels are included for species with values above the `r quantile * 100` % quantile (`r round(length(unique(prevalence_cv$species)) * (1 - quantile), 0)` species).

### Maximum absolute error

```{r, warning=FALSE, out.width="90%", message=FALSE}
# Calculate maximum error per species
prevalence_cv %>%
  mutate(max_error = max(abs_error),
         .by = c("species", "rarity")) %>%
  plot_cross_validation(
    prevalence_df,
    measure = "max_error",
    quant = quantile
  )
```

We look at the `r (1 - quantile) * 100` % species with the highest maximum absolute error.

```{r}
top_spec_max_error_df <- prevalence_cv %>%
  mutate(max_error = max(abs_error),
         .by = c("species", "rarity")) %>%
  distinct(species, rarity, max_error) %>%
  slice_max(max_error, prop = 1 - quantile) %>%
  arrange(desc(max_error))

top_spec_max_error_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_max_error_specs <- top_spec_max_error_df %>%
  slice_max(max_error, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_max_error_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_max_error_df, by = join_by(species)) %>%
  mutate(species = reorder(species, max_error, decreasing = TRUE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
  scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
  labs(x = "", y = "Number of observations (count)") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
  theme_bw(base_size = 12) +
  coord_flip() +
  facet_wrap(~species, ncol = 1, scales = "free")
```

### Maximum relative error

```{r, warning=FALSE, out.width="90%", message=FALSE}
# Calculate maximum relative error per species
prevalence_cv %>%
  mutate(max_rel_error = max(rel_error),
         .by = c("species", "rarity")) %>%
  plot_cross_validation(
    prevalence_df,
    measure = "max_rel_error",
    quant = quantile
  )
```

We look at the `r (1 - quantile) * 100` % species with the highest maximum relative error.

```{r}
top_spec_max_rel_error_df <- prevalence_cv %>%
  mutate(max_rel_error = max(rel_error),
         .by = c("species", "rarity")) %>%
  distinct(species, rarity, max_rel_error) %>%
  slice_max(max_rel_error, prop = 1 - quantile) %>%
  arrange(desc(max_rel_error))

top_spec_max_rel_error_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_max_rel_error_specs <- top_spec_max_rel_error_df %>%
  slice_max(max_rel_error, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_max_rel_error_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_max_rel_error_df, by = join_by(species)) %>%
  mutate(species = reorder(species, max_rel_error, decreasing = TRUE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
  scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
  labs(x = "", y = "Number of observations (count)") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
  theme_bw(base_size = 12) +
  coord_flip() +
  facet_wrap(~species, ncol = 1, scales = "free")
```

### Mean relative error

```{r, warning=FALSE, out.width="90%", message=FALSE}
# Mean relative error
plot_cross_validation(
  prevalence_cv,
  prevalence_df,
  measure = "mre",
  quant = quantile
)
```

```{r}
prevalence_cv %>%
  distinct(species, rarity, mre) %>%
  ggplot() +
  geom_histogram(aes(x = mre, fill = rarity)) +
  theme_bw(base_size = 12)
```

We look at the `r (1 - quantile) * 100` % species with the highest mean relative error (MRE).

```{r}
top_spec_mre_df <- prevalence_cv %>%
  distinct(species, rarity, mre) %>%
  slice_max(mre, prop = 1 - quantile) %>%
  arrange(desc(mre))

top_spec_mre_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_mre_specs <- top_spec_mre_df %>%
  slice_max(mre, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_mre_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_mre_df, by = join_by(species)) %>%
  mutate(species = reorder(species, mre, decreasing = TRUE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
  scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
  labs(x = "", y = "Number of observations (count)") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
  theme_bw(base_size = 12) +
  coord_flip() +
  facet_wrap(~species, ncol = 1, scales = "free")
```

We look at the `r (1 - quantile) * 100` % species with the lowest MRE.

```{r}
top_spec_mre_df <- prevalence_cv %>%
  distinct(species, rarity, mre) %>%
  slice_min(mre, prop = 1 - quantile) %>%
  arrange(mre)

top_spec_mre_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_mre_specs <- top_spec_mre_df %>%
  slice_min(mre, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_mre_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_mre_df, by = join_by(species)) %>%
  mutate(species = reorder(species, mre, decreasing = FALSE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
  scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
  labs(x = "", y = "Number of observations (count)") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
  theme_bw(base_size = 12) +
  coord_flip() +
  facet_wrap(~species, ncol = 1, scales = "free")
```

### Root mean squared error

```{r, warning=FALSE, out.width="90%", message=FALSE}
# Root mean squared error
plot_cross_validation(
  prevalence_cv,
  prevalence_df,
  measure = "rmse",
  quant = quantile
)
```

```{r}
prevalence_cv %>%
  distinct(species, rarity, rmse) %>%
  ggplot() +
  geom_histogram(aes(x = rmse, fill = rarity)) +
  theme_bw(base_size = 12)
```

We look at the `r (1 - quantile) * 100` % species with the highest RMSE.

```{r}
top_spec_rmse_df <- prevalence_cv %>%
  distinct(species, rarity, rmse) %>%
  slice_max(rmse, prop = 1 - quantile) %>%
  arrange(desc(rmse))

top_spec_rmse_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_rmse_specs <- top_spec_rmse_df %>%
  slice_max(rmse, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_rmse_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_rmse_df, by = join_by(species)) %>%
  mutate(species = reorder(species, rmse, decreasing = TRUE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
  scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
  labs(x = "", y = "Number of observations (count)") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
  theme_bw(base_size = 12) +
  coord_flip() +
  facet_wrap(~species, ncol = 1, scales = "free")
```

We look at the `r (1 - quantile) * 100` % species with the lowest RMSE.

```{r}
top_spec_rmse_df <- prevalence_cv %>%
  distinct(species, rarity, rmse) %>%
  slice_min(rmse, prop = 1 - quantile) %>%
  arrange(rmse)

top_spec_rmse_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_rmse_specs <- top_spec_rmse_df %>%
  slice_min(rmse, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_rmse_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_rmse_df, by = join_by(species)) %>%
  mutate(species = reorder(species, rmse, decreasing = FALSE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
  scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
  labs(x = "", y = "Number of observations (count)") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
  theme_bw(base_size = 12) +
  coord_flip() +
  facet_wrap(~species, ncol = 1, scales = "free")
```

## Trends in error

We look at trends in CV error measures related to:

1  differences in rarity
2. number of datasets
3. effective number of datasets
4. dataset evenness

### Differences in rarity

```{r}
prevalence_cv %>%
  ggplot(aes(x = rarity, y = rmse, colour = rarity)) +
  geom_boxplot() +
  theme_bw(base_size = 12)
```

### Number of datasets

```{r}
trend_dataset <- birdcube_dataset_filtered %>%
  left_join(prevalence_cv %>% distinct(species, rarity, rmse),
            by = join_by(species, rarity)) %>%
  select(mgrscode, year, species, datasetname, rarity, rmse) %>%
  count(species, rarity, rmse, datasetname) %>%
  group_by(species, rarity, rmse) %>%
  mutate(tot = sum(n),
         p = n / tot) %>%
  summarise(
    n_datasets = n_distinct(datasetname),
    shannon = -sum(p * log(p)),
    neff_datasets = exp(shannon),
    evenness = shannon / log(n_datasets),
    .groups = "drop"
  )
```

Does RMSE change with number of datasets?

```{r}
trend_dataset %>%
  ggplot(aes(x = n_datasets, y = rmse, colour = rarity)) +
  geom_point() +
  geom_smooth(method = "lm",  formula = "y ~ x") +
  theme_bw(base_size = 12)
```

### Effective number of datasets

The effective number of datasets takes into account the proportion of observations per dataset.
It is calculated per species $j$ as the exponent of the Shannon Entropy:

<!-- spell-check: ignore:start -->
$$
N_{E,j} = \exp(- \sum_{i=1}^{D_j}p_{ij}\ln(p_{ij}))
$$
<!-- spell-check: ignore:end -->

where $D_j$ the total number of datasets where species $j$ is present, and $p_i$ the proportion of entries (rows) of species $j$ in dataset $i$. 

```{r}
trend_dataset %>%
  ggplot(aes(x = neff_datasets, y = rmse, colour = rarity)) +
  geom_point() +
  geom_smooth(method = "lm",  formula = "y ~ x") +
  theme_bw(base_size = 12)
```

### Dataset evenness

Dataset evenness is a measure that captures how occurrences of a species are distributed across multiple datasets (0 is highly uneven, 1 is completely even).
Pielouâ€™s Evenness index $J$ is calculated as the normalised Shannon Entropy:

<!-- spell-check: ignore:start -->
$$
J_{j} = \frac{- \sum_{i=1}^{D_j}p_{ij}\ln(p_{ij}))}{\ln(D_j)}
$$
<!-- spell-check: ignore:end -->

```{r}
trend_dataset %>%
  ggplot(aes(x = evenness, y = rmse, colour = rarity)) +
  geom_point() +
  geom_smooth(method = "lm",  formula = "y ~ x") +
  theme_bw(base_size = 12)
```

# Species specific indicators

- LOO CV on species specific indicators
