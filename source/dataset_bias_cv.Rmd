---
title: "Investigate dataset bias"
author: "Ward Langeraert"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
    toc_collapsed: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
# Load packages
library(tidyverse) # Data wrangling and visualisation
library(dubicube)  # Cross-validation
library(sf)        # Spatial objects

# Source functions
source(here::here("source", "R", "download_occ_cube.R"))
source(here::here("source", "R", "get_dataset_names.R"))
source(here::here("source", "R", "plot_cross_validation.R"))
```

# Goal

Investigate bias in indicators caused by the over/under representation of certain datasets in the occurrence cube.

# Load data

We download the occurrence cube.
This cube is four dimensional, where the fourth dimension is the dataset.

```{r, message=FALSE}
# nolint start: line_length_linter.
query <- "SELECT
  \"year\",
  GBIF_MGRSCode(1000, decimalLatitude, decimalLongitude,
  COALESCE(coordinateUncertaintyInMeters, 1000)) AS mgrsCode,
  speciesKey,
  species,
  family,
  datasetName,
  datasetKey,
  COUNT(*) AS n,
  MIN(COALESCE(coordinateUncertaintyInMeters, 1000)) AS minCoordinateUncertaintyInMeters,
  IF(ISNULL(family), NULL, SUM(COUNT(*)) OVER (PARTITION BY family)) AS familyCount
  FROM
  occurrence
  WHERE
  occurrenceStatus = 'PRESENT'
  AND NOT ARRAY_CONTAINS(issue, 'ZERO_COORDINATE')
  AND NOT ARRAY_CONTAINS(issue, 'COORDINATE_OUT_OF_RANGE')
  AND NOT ARRAY_CONTAINS(issue, 'COORDINATE_INVALID')
  AND NOT ARRAY_CONTAINS(issue, 'COUNTRY_COORDINATE_MISMATCH')
  AND level1gid = 'BEL.2_1'
  AND \"year\" >= 2007
  AND \"year\" <= 2022
  AND speciesKey IS NOT NULL
  AND decimalLatitude IS NOT NULL
  AND decimalLongitude IS NOT NULL
  AND class = 'Aves'
  AND collectionCode != 'ABV'
  GROUP BY
  \"year\",
  mgrsCode,
  speciesKey,
  species,
  family,
  datasetName,
  datasetKey
  ORDER BY
  \"year\" ASC,
  mgrsCode ASC,
  speciesKey ASC,
  datasetName ASC"
# nolint end

download_occ_cube(
  sql_query = query,
  file = "birdcubeflanders_dataset.csv",
  overwrite = FALSE
)
```

> GBIF.org (04 March 2025) GBIF Occurrence Download https://doi.org/10.15468/dl.kmzpht

We read in the data cube and add dataset names.

```{r}
birdcubeflanders_dataset_raw <- read_csv(
  here::here("data", "interim", "birdcubeflanders_dataset.csv"),
  show_col_types = FALSE)

# Add dataset names
birdcubeflanders_dataset <- get_dataset_names(birdcubeflanders_dataset_raw) %>%
  dplyr::select("mgrscode", "year", "specieskey", "species", "family",
                "datasetkey", "datasetname", "n",
                "mincoordinateuncertaintyinmeters", "familycount") %>%
  arrange(year, mgrscode, species, datasetname)

glimpse(birdcubeflanders_dataset)
```

There are `r nrow(birdcubeflanders_dataset)` rows in the dataset.
They should be the same as the unique combinations of grid cell code, species, year and dataset.

```{r}
n_distinct <- birdcubeflanders_dataset %>%
  distinct(mgrscode, specieskey, year, datasetname) %>%
  nrow()

nrow(birdcubeflanders_dataset) == n_distinct
```

Number of observations (sum) per dataset in the cube.

```{r}
birdcubeflanders_dataset %>%
  summarise(n_obs = sum(n),
            .by = "datasetname") %>%
  arrange(desc(n_obs)) %>%
  knitr::kable()
```

```{r}
birdcubeflanders_dataset %>%
  summarise(n_obs = sum(n),
            .by = "datasetname") %>%
  mutate(datasetname = reorder(datasetname, n_obs)) %>%
  ggplot(aes(x = datasetname, y = n_obs)) +
    geom_bar(stat = "identity",
             fill = "cornflowerblue") +
    geom_text(aes(label = n_obs), vjust = 0.3, hjust = -0.3, size = 3) +
    scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
    scale_y_continuous(expand = expansion(mult = c(0.05, 0.2))) +
    labs(x = "", y = "Number of observations (sum)") +
    theme_minimal() +
    coord_flip()
```

# Comparing species prevalence
## Load and process data

We load the ABV data (structured monitoring data).
We categorise the species according to rarity:

| number of rows  | rarity           |
| :-------------: | :--------------- |
| [1, 10)         | very rare        |
| [10, 100)       | rare             |
| [100, 1000)     | common           |
| [1000, 10.000)  | very common      |
| [10.000, +Inf)  | extremely common |

```{r, message=FALSE}
abv_data_total_sf <- read_sf(
  here::here("data", "interim", "abv_data_total.gpkg"))

# Cut rarity
abv_data <- abv_data_total_sf %>%
  st_drop_geometry() %>%
  # Soepeend, Soepgans
  filter(!is.na(species)) %>%
  mutate(mgrscode = paste0("31U", TAG)) %>%
  dplyr::select("mgrscode", "year", "specieskey" = "speciesKey", "species") %>%
  mutate(n_obs = n(),
         .by = "species") %>%
  mutate(
    rarity = cut(
      n_obs,
      breaks = c(0, 10, 100, 1000, 10000, +Inf),
      labels = c("Very rare", "Rare", "Common",
                 "Very common", "Extremely common"),
      right = FALSE
    ),
    species = case_when(
      species == "Parus montanus" ~ "Poecile montanus",
      species == "Dendrocopus major" ~ "Dendrocopos major",
      species == "Saxicola torquatus" ~ "Saxicola rubicola",
      TRUE ~ species
      )
  ) %>%
  arrange(year, mgrscode, species)

glimpse(abv_data)
```

We filter the data cube based on species in ABV and add rarity categories.

```{r}
# Get ABV categories
abv_species <- unique(abv_data$species)

# Filter cube based on ABV species and add rarity categories
birdcube_dataset_filtered <- birdcubeflanders_dataset %>%
  dplyr::filter(species %in% abv_species) %>%
  left_join(distinct(abv_data[, c("species", "rarity")]),
            by = join_by("species"))

# Also aggregate over datasets
birdcube_filtered <- birdcube_dataset_filtered %>%
  group_by(mgrscode, year, specieskey, species, family, familycount, rarity) %>%
  summarise(
    n = sum(n),
    mincoordinateuncertaintyinmeters = min(mincoordinateuncertaintyinmeters),
    .groups = "drop"
  )
```

## Indicator calculation

We calculate species prevalence.
For each dataset we calculate the proportion of occupied grid cells by each species.

```{r}
species_prevalence <- function(
    df,
    total_grid_cells = length(unique(df$mgrscode))) {
  require("dplyr")
  require("rlang")

  df_out <- df %>%
    group_by(.data$species, .data$rarity) %>%
    summarise(diversity_val = n_distinct(.data$mgrscode) / total_grid_cells,
              .groups = "drop")

  return(df_out)
}
```

```{r}
birdcube_prevalence <- species_prevalence(birdcube_filtered) %>%
  mutate(data = "birdcube")
abv_prevalence <- species_prevalence(abv_data) %>%
  mutate(data = "abv")

prevalence_df <- bind_rows(birdcube_prevalence, abv_prevalence) %>%
  pivot_wider(names_from = data, values_from = diversity_val)
```

Rare species are more prevalent in the cube dataset while more common species are more prevalent in the ABV dataset.

```{r}
prevalence_df %>%
  ggplot(aes(x = abv, y = birdcube)) +
    geom_abline(slope = 1, intercept = 0, colour = "firebrick",
                linewidth = 1) +
    annotate("label", x = 0.8, y = 0.6, size = 3,
             label = "Higher prevalence in ABV",
             color = "black") +
    annotate("label", x = 0.4, y = 0.6, size = 3,
             label = "Higher prevalence in cube",
             color = "black") +
    geom_smooth(method = "loess", formula = "y ~ x",
                colour = "darkgrey", linetype = "dashed") +
    geom_point(aes(shape = rarity), size = 2) +
    labs(x = "Proportion of occupied grid cells\nin ABV dataset",
         y = "Proportion of occupied grid cells\nin cube dataset",
         shape = "Rarity") +
    theme_minimal()
```

We calculate error measures for the indicator based on leave-one-dataset-out cross-validation (use `remotes::install_github("b-cubed-eu/dubicube#25")`).
We use a constant total of grid cells (`r length(unique(birdcube_dataset_filtered$mgrscode))`) such that this is independent from the datasets left out.

```{r, message=FALSE}
prevalence_cv <- cross_validate_cube(
  birdcube_dataset_filtered,
  fun = species_prevalence,
  total_grid_cells = length(unique(birdcube_dataset_filtered$mgrscode)),
  grouping_var = c("species", "rarity"),
  out_var = "datasetkey",
  progress = TRUE
)
```

```{r}
quantile <- 0.95
```

We visualise several error measures per species. Labels are included for species with values above the `r quantile * 100` % quantile (`r round(length(unique(prevalence_cv$species)) * (1 - quantile), 0)` species).

### Maximum absolute error

```{r, warning=FALSE, out.width="90%", message=FALSE}
# Calculate maximum error per species
prevalence_cv %>%
  mutate(max_error = max(abs_error),
         .by = c("species", "rarity")) %>%
  plot_cross_validation(
    prevalence_df,
    measure = "max_error",
    quant = quantile)
```

We look at the `r (1 - quantile) * 100` % species with the highest maximum absolute error.

```{r}
top_spec_max_error_df <- prevalence_cv %>%
  mutate(max_error = max(abs_error),
         .by = c("species", "rarity")) %>%
  distinct(species, rarity, max_error) %>%
  slice_max(max_error, prop = 1 - quantile) %>%
  arrange(desc(max_error))

top_spec_max_error_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_max_error_specs <- top_spec_max_error_df %>%
  slice_max(max_error, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_max_error_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_max_error_df, by = join_by(species)) %>%
  mutate(species = reorder(species, max_error, decreasing = TRUE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
    scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
    labs(x = "", y = "Number of observations (count)") +
    scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
    theme_minimal() +
    coord_flip() +
    facet_wrap(~species, ncol = 1, scales = "free")
```

### Maximum relative error

```{r, warning=FALSE, out.width="90%", message=FALSE}
# Calculate maximum relative error per species
prevalence_cv %>%
  mutate(max_rel_error = max(rel_error),
         .by = c("species", "rarity")) %>%
  plot_cross_validation(
    prevalence_df,
    measure = "max_rel_error",
    quant = quantile)
```

We look at the `r (1 - quantile) * 100` % species with the highest maximum relative error.

```{r}
top_spec_max_rel_error_df <- prevalence_cv %>%
  mutate(max_rel_error = max(rel_error),
         .by = c("species", "rarity")) %>%
  distinct(species, rarity, max_rel_error) %>%
  slice_max(max_rel_error, prop = 1 - quantile) %>%
  arrange(desc(max_rel_error))

top_spec_max_rel_error_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_max_rel_error_specs <- top_spec_max_rel_error_df %>%
  slice_max(max_rel_error, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_max_rel_error_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_max_rel_error_df, by = join_by(species)) %>%
  mutate(species = reorder(species, max_rel_error, decreasing = TRUE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
    scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
    labs(x = "", y = "Number of observations (count)") +
    scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
    theme_minimal() +
    coord_flip() +
    facet_wrap(~species, ncol = 1, scales = "free")
```

### Mean relative error

```{r, warning=FALSE, out.width="90%", message=FALSE}
# Mean relative error
plot_cross_validation(
  prevalence_cv,
  prevalence_df,
  measure = "mre",
  quant = quantile)
```

```{r}
prevalence_cv %>%
  distinct(species, rarity, mre) %>%
  ggplot() +
    geom_histogram(aes(x = mre, fill = rarity))
```

We look at the `r (1 - quantile) * 100` % species with the highest maximum relative error.

```{r}
top_spec_mre_df <- prevalence_cv %>%
  distinct(species, rarity, mre) %>%
  slice_max(mre, prop = 1 - quantile) %>%
  arrange(desc(mre))

top_spec_mre_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_mre_specs <- top_spec_mre_df %>%
  slice_max(mre, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_mre_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_mre_df, by = join_by(species)) %>%
  mutate(species = reorder(species, mre, decreasing = TRUE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
    scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
    labs(x = "", y = "Number of observations (count)") +
    scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
    theme_minimal() +
    coord_flip() +
    facet_wrap(~species, ncol = 1, scales = "free")
```

### Root mean squared error

```{r, warning=FALSE, out.width="90%", message=FALSE}
# Root mean squared error
plot_cross_validation(
  prevalence_cv,
  prevalence_df,
  measure = "rmse",
  quant = quantile)
```

```{r}
prevalence_cv %>%
  distinct(species, rarity, rmse) %>%
  ggplot() +
    geom_histogram(aes(x = rmse, fill = rarity))
```

We look at the `r (1 - quantile) * 100` % species with the highest RMSE.

```{r}
top_spec_rmse_df <- prevalence_cv %>%
  distinct(species, rarity, rmse) %>%
  slice_max(rmse, prop = 1 - quantile) %>%
  arrange(desc(rmse))

top_spec_rmse_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_rmse_specs <- top_spec_rmse_df %>%
  slice_max(rmse, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_rmse_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_rmse_df, by = join_by(species)) %>%
  mutate(species = reorder(species, rmse, decreasing = TRUE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
    scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
    labs(x = "", y = "Number of observations (count)") +
    scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
    theme_minimal() +
    coord_flip() +
    facet_wrap(~species, ncol = 1, scales = "free")
```

We look at the `r (1 - quantile) * 100` % species with the lowest RMSE.

```{r}
top_spec_rmse_df <- prevalence_cv %>%
  distinct(species, rarity, rmse) %>%
  slice_min(rmse, prop = 1 - quantile) %>%
  arrange(rmse)

top_spec_rmse_df %>%
  knitr::kable(digits = 5)
```

```{r}
top_rmse_specs <- top_spec_rmse_df %>%
  slice_min(rmse, n = 5) %>%
  pull(species)

birdcube_dataset_filtered %>%
  dplyr::filter(species %in% top_rmse_specs) %>%
  count(species, datasetname) %>%
  left_join(top_spec_rmse_df, by = join_by(species)) %>%
  mutate(species = reorder(species, rmse, decreasing = FALSE)) %>%
  mutate(datasetname = tidytext::reorder_within(datasetname, n, species)) %>%
  ggplot(aes(x = datasetname, y = n)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label =  n), vjust = 0.3, hjust = -0.3, size = 3) +
    scale_x_discrete(label = function(x) stringr::str_trunc(x, 40)) +
    labs(x = "", y = "Number of observations (count)") +
    scale_y_continuous(expand = expansion(mult = c(0.05, 0.1))) +
    theme_minimal() +
    coord_flip() +
    facet_wrap(~species, ncol = 1, scales = "free")
```

## Trends in error

We look at trends in CV error measures related to:

1  differences in rarity
2. number of datasets
3. effective number of datasets
4. dataset evenness

### Differences in rarity

```{r}
prevalence_cv %>%
  ggplot(aes(x = rarity, y = rmse, colour = rarity)) +
    geom_boxplot()
```

### Number of datasets

```{r}
trend_dataset <- birdcube_dataset_filtered %>%
  left_join(prevalence_cv %>% distinct(species, rarity, rmse),
            by = join_by(species, rarity)) %>%
  select(mgrscode, year, species, datasetname, rarity, rmse) %>%
  count(species, rarity, rmse, datasetname) %>%
  group_by(species, rarity, rmse) %>%
  mutate(tot = sum(n),
         p = n / tot) %>%
  summarise(
    n_datasets = n_distinct(datasetname),
    shannon = -sum(p * log(p)),
    neff_datasets = exp(shannon),
    evenness = shannon / log(n_datasets),
    .groups = "drop")
```

Does RMSE change with number of datasets?

```{r}
trend_dataset %>%
  ggplot(aes(x = n_datasets, y = rmse, colour = rarity)) +
    geom_point() +
    geom_smooth(method = "lm",  formula = "y ~ x")
```

### Effective number of datasets

The effective number of datasets takes into account the proportion of observations per dataset.
It is calculated per species $j$ as the exponent of the Shannon Entropy:

<!-- spell-check: ignore:start -->
$$
N_{E,j} = \exp(- \sum_{i=1}^{D_j}p_{ij}\ln(p_{ij}))
$$
<!-- spell-check: ignore:end -->

where $D_j$ the total number of datasets where species $j$ is present, and $p_i$ the proportion of entries (rows) of species $j$ in dataset $i$. 

```{r}
trend_dataset %>%
  ggplot(aes(x = neff_datasets, y = rmse, colour = rarity)) +
    geom_point() +
    geom_smooth(method = "lm",  formula = "y ~ x")
```

### Dataset evenness

Dataset evenness is a measure that captures how occurrences of a species are distributed across multiple datasets (0 is highly uneven, 1 is completely even).
Pielouâ€™s Evenness index $J$ is calculated as the normalised Shannon Entropy:

<!-- spell-check: ignore:start -->
$$
J_{j} = \frac{- \sum_{i=1}^{D_j}p_{ij}\ln(p_{ij}))}{\ln(D_j)}
$$
<!-- spell-check: ignore:end -->

```{r}
trend_dataset %>%
  ggplot(aes(x = evenness, y = rmse, colour = rarity)) +
    geom_point() +
    geom_smooth(method = "lm",  formula = "y ~ x")
```

# Species specific indicators

- LOO CV on species specific indicators
